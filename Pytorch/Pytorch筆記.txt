Pytorch: Use tensor as base unit to build deep learning framework
Dynamic graph: 和tensorflow的static graph不同,
 Static graph: 需先定義session, placeholder. 運算的graph為是先定義的封閉環境, 先定義好候用feed_dict輸入資料來溝通
 dynamic graph: 不須session等, tensor可直接用變數方式定義，隨時變更長度等
 
 Dynamic graph再input不等長時方便隨時修改, static graph定義好後輸入的大小為固定


Note: most of the methods (datasets, models) recommends to create custom class obejct 
that inherit the parent modules:

class MyDataset(torch.utils.data.Dataset):

class MyModel(torch.nn.Module):




Start: 
import torch

Create tensor: torch.tensor()


Build neural network layer:
   torch.nn



Import pretrained models:
   torch.hub.load()
   torchvision.models


2021.2.19

Decanter AI: for multiple models, we could combine and normalize the results to give an overall feature importance.
Model stability: observe the standard deviation of model performance.

Some features in PyTorch:

.to(device)
In pytorch, when we have GPU, it's better to set the model and dataset on same device(both on GPU or CPU) or the model might invoke RuntimeError when accessing the data.
PyTorch by default set both model and data on CPU, se when we have GPU for speed up, we have to move the Tensor(model and dataset) to GPU device to have the model run on GPU.
Use torch.nn.Module.to() or torch.Tensor.to()
Ex: 
  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
  model = model.to(device)
  data = data.to(device)

parameter.requires_grad: set neural network layer's parameter requires_grad = True, means that we would compute gradient for these layers, implying these parameters in layers should be optimzied.
  On the contrary, if we have some layers that weight need to be fixed, we could set these layers.requires_grad = False to save memory and preserve the already desired layer. 
  We could later pass these parameters which have gradients to the optimizer.

PyTorch tensor shape: (batch, channels, height, width)

Adaptive Average Pooling: set up output size, then the adapAvgPooling layer would set up stride step and kernel size to fit our needs. 
Algorithm:
Stride = (input_size//output_size)  
Kernel size = input_size - (output_size-1)*stride  
Padding = 0
